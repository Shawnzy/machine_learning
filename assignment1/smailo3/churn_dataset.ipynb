{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "main_file_path = './data_customer.csv'\n",
    "data = pd.read_csv(main_file_path)\n",
    "\n",
    "print('Number of Samples: ', data.shape[0])\n",
    "print('Number of Attributes: ', data.shape[1]-1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Data types of each column\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable TotalCharges should be a continuous numeric data type rather than a discrete object data type\n",
    "data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')\n",
    "\n",
    "# Check for NULL entries\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all entries that contain NULL\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Remove customer ID\n",
    "df = data.iloc[:,1:]\n",
    "\n",
    "# Separate X and Y\n",
    "ydata = df.Churn #df_upsampled.Churn\n",
    "xdata_raw = df.drop(['Churn'], axis=1) #df_upsampled.drop(['Churn'], axis=1)\n",
    "\n",
    "# Convert Chrun into binary numeric variable\n",
    "ydata.replace(to_replace='Yes', value=1, inplace=True)\n",
    "ydata.replace(to_replace='No', value=0, inplace=True)\n",
    "\n",
    "# Convert all categorial variables in xdata into dummy variables\n",
    "xdata = pd.get_dummies(xdata_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class0 = np.where(ydata == 0)[0]\n",
    "i_class1 = np.where(ydata == 1)[0]\n",
    "print(\"If choose 0 for all guesses: \", len(i_class0)/data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.countplot(ydata,label=\"Count\")       \n",
    "Not_Churned, Churned = ydata.value_counts()\n",
    "print('Number Churned: ', Churned)\n",
    "print('Number Not Churned: ',Not_Churned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into Train and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(xdata, ydata, test_size=.2, random_state=101, stratify=ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all features from 0 to 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = X_train.columns.values\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "X_train.columns = features\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "X_test.columns = features\n",
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Models - Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def grid_search(estimator, param_grid, X_train, y_train):\n",
    "    grid = GridSearchCV(estimator, param_grid, cv=10, scoring='accuracy', return_train_score=True)\n",
    "    grid.fit(X_train, y_train)\n",
    "#     print(pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "\n",
    "#     print()\n",
    "    # examine the best model\n",
    "    print('Best_score = ', grid.best_score_)\n",
    "    print('Best_params = ', grid.best_params_)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid={\n",
    "             'criterion': [\"gini\", \"entropy\"],\n",
    "             'max_depth': range(2,21,2),\n",
    "           }\n",
    "\n",
    "dtree_grid = grid_search(DecisionTreeClassifier(), param_grid, X_train, y_train)\n",
    "joblib.dump(dtree_grid, 'dtree_grid.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Nerual Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "param_grid = {\n",
    "               'solver': ['lbfgs','sgd','adam'],\n",
    "               'alpha': 10.0 ** -np.arange(1, 5),\n",
    "               'hidden_layer_sizes':[1,2,5,10], \n",
    "               'random_state':[0,4,8]\n",
    "             }\n",
    "\n",
    "nn_grid = grid_search(MLPClassifier(), param_grid, X_train, y_train)\n",
    "joblib.dump(nn_grid, 'nn_grid.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid = {\n",
    "               'loss': ['deviance', 'exponential'],    \n",
    "               'learning_rate': [0.05, 0.1, 0.2],\n",
    "               'max_depth': [2,5,10,15,20],\n",
    "               'min_samples_split': [2,5,10,15]\n",
    "             }\n",
    "\n",
    "gb_grid = grid_search(GradientBoostingClassifier(random_state = 0), param_grid, X_train, y_train)\n",
    "joblib.dump(gb_grid, 'gb_grid.pkl')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = [\n",
    "               {\n",
    "                 'C': [1, 10, 100, 1000], \n",
    "                 'kernel': ['linear']\n",
    "               },\n",
    "               {\n",
    "                 'C': [1, 10, 100, 1000], \n",
    "                 'gamma': [0.001, 0.0001], \n",
    "                 'kernel': ['rbf']\n",
    "               },\n",
    "             ]\n",
    "\n",
    "svc_grid = grid_search(SVC(), param_grid, X_train, y_train)\n",
    "joblib.dump(svc_grid, 'svc_grid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **K Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {\n",
    "               'n_neighbors': range(1, 31),    \n",
    "               'weights': ['uniform', 'distance']\n",
    "             }\n",
    "\n",
    "knn_grid = grid_search(KNeighborsClassifier(), param_grid, X_train, y_train)\n",
    "joblib.dump(knn_grid, 'knn_grid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Models - Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load trained models\n",
    "dtree_grid = joblib.load('dtree_grid.pkl') \n",
    "nn_grid = joblib.load('nn_grid.pkl') \n",
    "gb_grid = joblib.load('gb_grid.pkl') \n",
    "svc_grid = joblib.load('svc_grid.pkl') \n",
    "knn_grid = joblib.load('knn_grid.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, estimator_name, X, y, ylim=None, cv=10, train_sizes=[19,50,100,200,300,400]):\n",
    "    plt.figure()\n",
    "    param_str = ', '.join('\\'{}\\': {}'.format(key, val) for key, val in estimator.best_params_.items())\n",
    "    plt.title(estimator_name + ' ' + \"Learning Curves\")\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator.best_estimator_, X, y, cv=cv, scoring='accuracy', train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig('Learning_curve-{}.png'.format(estimator_name))\n",
    "    plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "plot_learning_curve(dtree_grid, 'Decision_Tree', X_train, y_train, (0.7,1.01), train_sizes=[19,50,100,220,300,409])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nueral Network\n",
    "plot_learning_curve(nn_grid, 'Neural_Network', X_train, y_train, (0.7,1.01), cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting\n",
    "plot_learning_curve(gb_grid, 'Boosting', X_train, y_train, (0.7,1.01), cv=10, train_sizes=[60,100,200,300,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "plot_learning_curve(svc_grid, 'Support_Vector_Machine', X_train, y_train, (0.7,1.01), cv=10, train_sizes=[60,100,200,300,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "plot_learning_curve(knn_grid, 'K-Nearest_Neighbors', X_train, y_train, (0.7,1.01), cv=10, train_sizes=[24,50,100,220,300,409])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity Curves - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "def complexity_curve(estimator, title, param_name, param_range, X=X_train,y=y_train):\n",
    "    train_scores, test_scores = validation_curve(estimator.best_estimator_, X, y, param_name=param_name, param_range=param_range,cv=10, scoring=\"accuracy\")\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.title(\"Complexity Curve with \" + title)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.ylim(0.6, 1.05)\n",
    "\n",
    "    plt.plot(param_range, train_scores_mean,'o-', label=\"Training score\",\n",
    "                 color=\"r\")\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.plot(param_range, test_scores_mean,'o-', label=\"Cross-validation score\",\n",
    "                 color=\"g\")\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                     color=\"g\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Complexity_curve-{}.png'.format(title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "complexity_curve(dtree_grid, 'Decsion_tree', 'max_depth', [1,2,3,4,5,7,8,9,10,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "complexity_curve(nn_grid, 'Neural_Network', 'hidden_layer_sizes', [1,3,6,10,13,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "complexity_curve(gb_grid, 'Boosting', 'max_depth', [1,2,4,6,8,10,12,16,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "complexity_curve(svc_grid, 'Support_Vector_Machine', 'gamma', [.000001,.00001,.0001,.001,.002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "complexity_curve(knn_grid, 'K-Nearest_Neighbors', 'n_neighbors', [1,5,10,15,19,25,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves - Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "title = 'Neural_Network'\n",
    "iterations= [1,10,25,50,100,125]\n",
    "fit_times = []\n",
    "pred_times = []\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for iteration in iterations: \n",
    "    estimator = MLPClassifier(random_state=6,hidden_layer_sizes=10,solver='adam',alpha=0.1, max_iter=iteration)\n",
    "    start_time = timeit.default_timer()\n",
    "    estimator.fit(X_train, y_train) \n",
    "    fit_times.append(timeit.default_timer() - start_time)\n",
    "\n",
    "    start_time1 = timeit.default_timer()\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    pred_times.append(timeit.default_timer() - start_time1)\n",
    "    test_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    y_pred = estimator.predict(X_train)\n",
    "    train_scores.append(accuracy_score(y_train, y_pred))\n",
    "\n",
    "\n",
    "plt.figure(0)\n",
    "plt.grid()\n",
    "plt.plot(iterations, train_scores,'o-', label=\"Training score\", color=\"r\")\n",
    "plt.plot(iterations, test_scores,'o-', label=\"Testing score\", color=\"g\")\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(title + ' Interation Learning Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig('Iteration_Score_curve-{}.png'.format(title))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.grid()\n",
    "plt.plot(iterations, fit_times,'o-', label=\"Fit Time\", color=\"b\")\n",
    "plt.plot(iterations, pred_times,'o-', label=\"Prediction Time\", color=\"g\")\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(title + ' Interation Learning Curve - Fit and Prediction Times')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig('Iteration_Time_curve-{}.png'.format(title))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "title = 'Boosting'\n",
    "iterations= [1,10,25,50,100,550]\n",
    "fit_times = []\n",
    "pred_times = []\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for iteration in iterations: \n",
    "    estimator = GradientBoostingClassifier(min_samples_split=15, loss='deviance', learning_rate=0.2, max_depth=4, n_estimators=iteration)\n",
    "    start_time = timeit.default_timer()\n",
    "    estimator.fit(X_train, y_train) \n",
    "    fit_times.append(timeit.default_timer() - start_time)\n",
    "\n",
    "    start_time1 = timeit.default_timer()\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    pred_times.append(timeit.default_timer() - start_time1)\n",
    "    test_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    y_pred = estimator.predict(X_train)\n",
    "    train_scores.append(accuracy_score(y_train, y_pred))\n",
    "\n",
    "\n",
    "plt.figure(0)\n",
    "plt.grid()\n",
    "plt.plot(iterations, train_scores,'o-', label=\"Training score\", color=\"r\")\n",
    "plt.plot(iterations, test_scores,'o-', label=\"Testing score\", color=\"g\")\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(title + ' Interation Learning Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig('Iteration_Score_curve-{}.png'.format(title))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.grid()\n",
    "plt.plot(iterations, fit_times,'o-', label=\"Fit Time\", color=\"b\")\n",
    "plt.plot(iterations, pred_times,'o-', label=\"Prediction Time\", color=\"g\")\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(title + ' Interation Learning Curve - Fit and Prediction Times')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig('Iteration_Time_curve-{}.png'.format(title))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "title = 'Support_Vector_Machine'\n",
    "iterations= [1,20,70,100]\n",
    "fit_times = []\n",
    "pred_times = []\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for iteration in iterations: \n",
    "    estimator = SVC(C=1000,gamma=0.001,kernel='rbf', max_iter=iteration)\n",
    "    start_time = timeit.default_timer()\n",
    "    estimator.fit(X_train, y_train) \n",
    "    fit_times.append(timeit.default_timer() - start_time)\n",
    "\n",
    "    start_time1 = timeit.default_timer()\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    pred_times.append(timeit.default_timer() - start_time1)\n",
    "    test_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    y_pred = estimator.predict(X_train)\n",
    "    train_scores.append(accuracy_score(y_train, y_pred))\n",
    "\n",
    "\n",
    "plt.figure(0)\n",
    "plt.grid()\n",
    "plt.plot(iterations, train_scores,'o-', label=\"Training score\", color=\"r\")\n",
    "plt.plot(iterations, test_scores,'o-', label=\"Testing score\", color=\"g\")\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(title + ' Interation Learning Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig('Iteration_Score_curve-{}.png'.format(title))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.grid()\n",
    "plt.plot(iterations, fit_times,'o-', label=\"Fit Time\", color=\"b\")\n",
    "plt.plot(iterations, pred_times,'o-', label=\"Prediction Time\", color=\"g\")\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title(title + ' Interation Learning Curve - Fit and Prediction Times')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig('Iteration_Time_curve-{}.png'.format(title))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_test_score(estimator, title):\n",
    "    start_time = timeit.default_timer()\n",
    "    estimator.fit(X_train, y_train) \n",
    "    print(title + ' Fit Time: ',round(timeit.default_timer() - start_time,5))\n",
    "\n",
    "    start_time1 = timeit.default_timer()\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    print(title + ' Prediction Time: ',round(timeit.default_timer() - start_time1,5))\n",
    "    print(title + ' Final Test Score: ',round(accuracy_score(y_test, y_pred)*100,1),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_score(dtree_grid.best_estimator_, 'Decision_Tree')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_score(nn_grid.best_estimator_, 'Neural_Network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_score(gb_grid.best_estimator_, 'Boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_score(svc_grid.best_estimator_, 'Support_Vector_Machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -  K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_score(knn_grid.best_estimator_, 'K-Nearest_Neighbors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
